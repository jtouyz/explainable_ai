{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600863278673",
   "display_name": "Python 3.7.3 64-bit ('p1_venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME - Summary\n",
    "# Summary\n",
    "- Local Interpretable Model-agnostic Explanations or LIME is a model agnostic machine learning technique used to provide individual level explanations for individual instances of a data set.\n",
    "- Developed in the context of classification\n",
    "- Use for text, image, and tabular data\n",
    "## History\n",
    "- Proposed in 2016 by Riberio et al in \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier\"\n",
    "- \n",
    "\n",
    "## Why we use it\n",
    "- Goal: understand why a machine learning model made a certain prediction for a given instance\n",
    "\n",
    "## What it used for\n",
    "# How it works\n",
    "- To use LIME we make a couple of assumptions\n",
    "    - Imagine you have a black box model where you can input data points to solicit preidtions from your model\n",
    "    - This mechanism can be used as frequently as you would like to probe the model\n",
    "- Approach:\n",
    "    - For a given instance, generate a new set of data by locally perturbing/the sample input\n",
    "    - Feed in the synthetic data into the black box (trained) model and generate a predicted class\n",
    "    - Use a distance measure from the instance class to weight the data to a the instance of interest\n",
    "    - Build a local model with intrinsic intepretability, such as a linear classifier, to explain the prediction of the classifier\n",
    "- A local surrogate model with interpretability constraint is expressed as follows:\n",
    "\n",
    "$$\n",
    "\\mbox{explaination}(x) = \\arg\\min_{g\\in G} L(f,g,\\pi_x) + \\Omega(g)\n",
    "$$\n",
    "where:\n",
    "- $L$ is the loss measure, i.e. mean squared error\n",
    "- $f$ is the original model, i.e. xgboost\n",
    "- $g$ is the local linear model\n",
    "- $G$ is the family of possible models, i.e all linear classigiers\n",
    "- $\\pi_x$ is the proximity measure\n",
    "- $\\Omega(g)$ is the model complexity\n",
    "\n",
    "The explanation model for instance $x$ is the model  $g$ that minimizes the lo\n",
    "\n",
    "### Notes on implementation\n",
    "- In general we optimize the loss function, the model complexity is usually selected by the modeler\n",
    "- Implementations of LIME exist in R and python:\n",
    "    - R implementation: https://github.com/thomasp85/lime\n",
    "    - Python implementation: https://github.com/marcotcr/lime\n",
    "- Using LASSO is usually a \n",
    "\n",
    "### Modes of pertubation for local data\n",
    "- For images turn super-pixels on or off\n",
    "- For text add/remove words\n",
    "- For tabular data: locally perturbs each feature individually by drawing a sample with mean and standard deviation taken from the feature\n",
    "\n",
    "# Pros/cons\n",
    "- Pros:\n",
    "    - Easy to interpret in the sense that you layer an explainable model onto other models, for example if you train an SVM and find a NN works better you do not need to change the top layer surrogate model but rather only the backend\n",
    "    - Benefit from history of interpretable models\n",
    "    - When using LASSO or short tress the resutling explanations are short which has the benefit of ease of explanability for lay people\n",
    "\n",
    "TO DO: discussion of fidelity measure\n",
    "TO DO: discussino of feature selection and limitations of data types\n",
    "\n",
    "- Cons:  \n",
    "    - It does not provide complete attribution and may be a challenge in compliance scenarios here full explanations are required for a given prediction\n",
    "    - The short \n",
    "    - Weighting kenrel is heuristic\n",
    "    - For each instance need to generate perturbed data set\n",
    "    - The user needs to select the model complexity (i.e. number of parameters $k$ in a regression model)\n",
    "    - Instability of the explanations occurs, i.e. when points are resampled there may be vastly different explanations\n",
    "        - One should have a critical eye of the explanation\n",
    "\n",
    "# Applications + examples\n",
    "- TO DO\n",
    "\n",
    "# Improvements\n",
    "- Principled data selection \n",
    "- Principled kernel selection to assess different levels of accuracy\n",
    "- Constrained kernel to increase speed\n",
    "\n",
    "# References\n",
    "- https://www.danielemaasit.com/blog/lime_image/\n",
    "- https://nbviewer.jupyter.org/urls/arteagac.github.io/blog/lime.ipynb\n",
    "- Original article: https://arxiv.org/abs/1602.04938\n",
    "- Example of LIME instability: https://christophm.github.io/interpretable-ml-book/lime.html#fn35 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}